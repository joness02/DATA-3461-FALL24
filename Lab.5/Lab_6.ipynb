{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "49cf8381",
      "metadata": {
        "id": "49cf8381"
      },
      "source": [
        "# Lab 6\n",
        "\n",
        "Scikit learn provides a large variety of algorithms for some common Machine Learning tasks, such as:\n",
        "\n",
        "* Classification\n",
        "* Regression\n",
        "* Clustering\n",
        "* Feature Selection\n",
        "* Anomaly Detection\n",
        "\n",
        "It also provides some datasets that you can use to test these algorithms:\n",
        "\n",
        "* Classification Datasets:\n",
        "    * Breast cancer wisconsin\n",
        "    * Iris plants (3-classes)\n",
        "    * Optical recognition of handwritten digits (10-classes)\n",
        "    * Wine (n-classes)\n",
        "\n",
        "* Regression Datasets:\n",
        "    * Boston house prices\n",
        "    * Diabetes\n",
        "    * Linnerrud (multiple regression)\n",
        "    * California Housing\n",
        "\n",
        "* Image:\n",
        "    * The Olivetti faces\n",
        "    * The Labeled Faces in the Wild face recognition\n",
        "    * Forest covertypes\n",
        "\n",
        "* NLP:\n",
        "    * News group\n",
        "    * Reuters Corpus Volume I\n",
        "\n",
        "* Other:\n",
        "    * Kddcup 99- Intrusion Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4e3b6e9",
      "metadata": {
        "id": "d4e3b6e9"
      },
      "source": [
        "##### Exercises\n",
        "\n",
        "1. Use the full [Kddcup](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) dataset to compare classification performance of 3 different classifiers.\n",
        "    * Separate the data into train, validation, and test.\n",
        "    * Use accuracy as the metric for assessing performance.\n",
        "    * For each classifier, identify the hyperparameters. Perform optimization over at least 2 hyperparameters.   \n",
        "    * Compare the performance of the optimal configuration of the classifiers.\n",
        "\n",
        "2. Pick the best algorithm in question 1. Create an ensemble of at least 25 models, and use them for the classification task. Identify the top and bottom 10% of the data in terms of uncertainty of the decision.\n",
        "\n",
        "3. Use 2 different feature selection algorithm to identify the 10 most important features for the task in question 1. Retrain classifiers in question 1 with just this subset of features and compare performance.\n",
        "\n",
        "4. Use the same data, removing the labels, and compare performance of 3 different clustering algorithms. Can you find clusters for each of the classes in question 1?\n",
        "\n",
        "5. Can you identify any clusters within the top/botton 10% identified in 2. What are their characteristics?\n",
        "\n",
        "6. Use the \"SA\" dataset to compare the performance of 3 different anomaly detection algorithms.\n",
        "\n",
        "7. Create a subsample of 250 datapoints, redo question 6, using Leave-one-out as the method of evaluation.\n",
        "\n",
        "8. Use the feature selection algorithm to identify the 5 most important features for the task in question 6, for each algorithm. Does the anomaly detection improve using less features?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef18662c",
      "metadata": {
        "id": "ef18662c"
      },
      "source": [
        "## Quick look at the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9f1c631a",
      "metadata": {
        "id": "9f1c631a"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_kddcup99\n",
        "D=fetch_kddcup99()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "7d561eff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d561eff",
        "outputId": "35a91efd-255c-45e1-8db5-3a122ed3c0ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "dir(D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "875d2d16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "875d2d16",
        "outputId": "e036a4fb-7345-4586-dd35-99fa54240bf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _kddcup99_dataset:\n",
            "\n",
            "Kddcup 99 dataset\n",
            "-----------------\n",
            "\n",
            "The KDD Cup '99 dataset was created by processing the tcpdump portions\n",
            "of the 1998 DARPA Intrusion Detection System (IDS) Evaluation dataset,\n",
            "created by MIT Lincoln Lab [2]_. The artificial data (described on the `dataset's\n",
            "homepage <https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html>`_) was\n",
            "generated using a closed network and hand-injected attacks to produce a\n",
            "large number of different types of attack with normal activity in the\n",
            "background. As the initial goal was to produce a large training set for\n",
            "supervised learning algorithms, there is a large proportion (80.1%) of\n",
            "abnormal data which is unrealistic in real world, and inappropriate for\n",
            "unsupervised anomaly detection which aims at detecting 'abnormal' data, i.e.:\n",
            "\n",
            "* qualitatively different from normal data\n",
            "* in large minority among the observations.\n",
            "\n",
            "We thus transform the KDD Data set into two different data sets: SA and SF.\n",
            "\n",
            "* SA is obtained by simply selecting all the normal data, and a small\n",
            "  proportion of abnormal data to gives an anomaly proportion of 1%.\n",
            "\n",
            "* SF is obtained as in [3]_\n",
            "  by simply picking up the data whose attribute logged_in is positive, thus\n",
            "  focusing on the intrusion attack, which gives a proportion of 0.3% of\n",
            "  attack.\n",
            "\n",
            "* http and smtp are two subsets of SF corresponding with third feature\n",
            "  equal to 'http' (resp. to 'smtp').\n",
            "\n",
            "General KDD structure:\n",
            "\n",
            "================      ==========================================\n",
            "Samples total         4898431\n",
            "Dimensionality        41\n",
            "Features              discrete (int) or continuous (float)\n",
            "Targets               str, 'normal.' or name of the anomaly type\n",
            "================      ==========================================\n",
            "\n",
            "SA structure:\n",
            "\n",
            "================      ==========================================\n",
            "Samples total         976158\n",
            "Dimensionality        41\n",
            "Features              discrete (int) or continuous (float)\n",
            "Targets               str, 'normal.' or name of the anomaly type\n",
            "================      ==========================================\n",
            "\n",
            "SF structure:\n",
            "\n",
            "================      ==========================================\n",
            "Samples total         699691\n",
            "Dimensionality        4\n",
            "Features              discrete (int) or continuous (float)\n",
            "Targets               str, 'normal.' or name of the anomaly type\n",
            "================      ==========================================\n",
            "\n",
            "http structure:\n",
            "\n",
            "================      ==========================================\n",
            "Samples total         619052\n",
            "Dimensionality        3\n",
            "Features              discrete (int) or continuous (float)\n",
            "Targets               str, 'normal.' or name of the anomaly type\n",
            "================      ==========================================\n",
            "\n",
            "smtp structure:\n",
            "\n",
            "================      ==========================================\n",
            "Samples total         95373\n",
            "Dimensionality        3\n",
            "Features              discrete (int) or continuous (float)\n",
            "Targets               str, 'normal.' or name of the anomaly type\n",
            "================      ==========================================\n",
            "\n",
            ":func:`sklearn.datasets.fetch_kddcup99` will load the kddcup99 dataset; it\n",
            "returns a dictionary-like object with the feature matrix in the ``data`` member\n",
            "and the target values in ``target``. The \"as_frame\" optional argument converts\n",
            "``data`` into a pandas DataFrame and ``target`` into a pandas Series. The\n",
            "dataset will be downloaded from the web if necessary.\n",
            "\n",
            ".. rubric:: References\n",
            "\n",
            ".. [2] Analysis and Results of the 1999 DARPA Off-Line Intrusion\n",
            "       Detection Evaluation, Richard Lippmann, Joshua W. Haines,\n",
            "       David J. Fried, Jonathan Korba, Kumar Das.\n",
            "\n",
            ".. [3] K. Yamanishi, J.-I. Takeuchi, G. Williams, and P. Milne. Online\n",
            "       unsupervised outlier detection using finite mixtures with\n",
            "       discounting learning algorithms. In Proceedings of the sixth\n",
            "       ACM SIGKDD international conference on Knowledge discovery\n",
            "       and data mining, pages 320-324. ACM Press, 2000.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(D[\"DESCR\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f3c3c5b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3c3c5b8",
        "outputId": "1aebb58c-f4da-4adc-ae9f-0864e2e18b47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DESCR', 'data', 'feature_names', 'frame', 'target', 'target_names']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "dir(D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4cef559d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cef559d",
        "outputId": "f4ac5b53-cca6-47cf-bf7a-cc97bc9f57dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'back.', b'buffer_overflow.', b'ftp_write.', b'guess_passwd.',\n",
              "       b'imap.', b'ipsweep.', b'land.', b'loadmodule.', b'multihop.',\n",
              "       b'neptune.', b'nmap.', b'normal.', b'perl.', b'phf.', b'pod.',\n",
              "       b'portsweep.', b'rootkit.', b'satan.', b'smurf.', b'spy.',\n",
              "       b'teardrop.', b'warezclient.', b'warezmaster.'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.unique(D[\"target\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6ed0289b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ed0289b",
        "outputId": "6fa948b1-ae22-48cb-d7dc-4d6fdc4e523a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(np.unique(D[\"target\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "aff034ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aff034ea",
        "outputId": "9fa38a19-3964-485f-cfb6-bbfba2f2306b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['duration',\n",
              " 'protocol_type',\n",
              " 'service',\n",
              " 'flag',\n",
              " 'src_bytes',\n",
              " 'dst_bytes',\n",
              " 'land',\n",
              " 'wrong_fragment',\n",
              " 'urgent',\n",
              " 'hot',\n",
              " 'num_failed_logins',\n",
              " 'logged_in',\n",
              " 'num_compromised',\n",
              " 'root_shell',\n",
              " 'su_attempted',\n",
              " 'num_root',\n",
              " 'num_file_creations',\n",
              " 'num_shells',\n",
              " 'num_access_files',\n",
              " 'num_outbound_cmds',\n",
              " 'is_host_login',\n",
              " 'is_guest_login',\n",
              " 'count',\n",
              " 'srv_count',\n",
              " 'serror_rate',\n",
              " 'srv_serror_rate',\n",
              " 'rerror_rate',\n",
              " 'srv_rerror_rate',\n",
              " 'same_srv_rate',\n",
              " 'diff_srv_rate',\n",
              " 'srv_diff_host_rate',\n",
              " 'dst_host_count',\n",
              " 'dst_host_srv_count',\n",
              " 'dst_host_same_srv_rate',\n",
              " 'dst_host_diff_srv_rate',\n",
              " 'dst_host_same_src_port_rate',\n",
              " 'dst_host_srv_diff_host_rate',\n",
              " 'dst_host_serror_rate',\n",
              " 'dst_host_srv_serror_rate',\n",
              " 'dst_host_rerror_rate',\n",
              " 'dst_host_srv_rerror_rate']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "D[\"feature_names\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfabfc31",
      "metadata": {
        "id": "cfabfc31"
      },
      "source": [
        "# Exercise 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_kddcup99\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = fetch_kddcup99()\n",
        "X = pd.DataFrame(data.data)  # Features\n",
        "y = pd.Series(data.target)   # Target\n",
        "\n",
        "# Decode byte strings in categorical columns\n",
        "X = X.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
        "y = y.apply(lambda x: x.decode() if isinstance(x, bytes) else x)\n",
        "\n",
        "# Preprocessing: Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "numerical_cols = X.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "# Preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_cols),\n",
        "        ('cat', OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Step 2: Split the data into train, validation, and test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 3: Define classifiers and hyperparameters\n",
        "# 1. Decision Tree\n",
        "dt_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
        "])\n",
        "dt_params = {\n",
        "    'classifier__max_depth': [10, 20, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 2. Random Forest\n",
        "rf_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "rf_params = {\n",
        "    'classifier__n_estimators': [50, 100, 200],\n",
        "    'classifier__max_depth': [10, 20, None]\n",
        "}\n",
        "\n",
        "# Step 4: Hyperparameter tuning with GridSearchCV\n",
        "# Function to perform grid search and output best model and parameters\n",
        "def tune_and_evaluate(pipeline, param_grid, X_train, y_train, X_val, y_val):\n",
        "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    val_predictions = best_model.predict(X_val)\n",
        "    val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "    return best_model\n",
        "\n",
        "# Tune each classifier\n",
        "print(\"Tuning Decision Tree...\")\n",
        "best_dt = tune_and_evaluate(dt_pipeline, dt_params, X_train, y_train, X_val, y_val)\n",
        "\n",
        "print(\"\\nTuning Random Forest...\")\n",
        "best_rf = tune_and_evaluate(rf_pipeline, rf_params, X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Step 5: Evaluate on the test set\n",
        "def evaluate_on_test(model, X_test, y_test, model_name):\n",
        "    test_predictions = model.predict(X_test)\n",
        "    test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "    print(f\"{model_name} Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nEvaluating on Test Set...\")\n",
        "evaluate_on_test(best_dt, X_test, y_test, \"Decision Tree\")\n",
        "evaluate_on_test(best_rf, X_test, y_test, \"Random Forest\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fZ7VCmWm5PJ",
        "outputId": "03656952-84ea-4632-a8ad-b81631e3a44c"
      },
      "id": "4fZ7VCmWm5PJ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-2de5558b87dd>:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  X = X.applymap(lambda x: x.decode() if isinstance(x, bytes) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning Decision Tree...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__max_depth': None, 'classifier__min_samples_split': 2}\n",
            "Validation Accuracy: 0.9996\n",
            "\n",
            "Tuning Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'classifier__max_depth': None, 'classifier__n_estimators': 200}\n",
            "Validation Accuracy: 0.9997\n",
            "\n",
            "Evaluating on Test Set...\n",
            "Decision Tree Test Accuracy: 0.9997\n",
            "Random Forest Test Accuracy: 0.9997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37234ed9",
      "metadata": {
        "id": "37234ed9"
      },
      "source": [
        "# Exercise 2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_kddcup99\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# Load the KDDcup99 dataset\n",
        "data = fetch_kddcup99()\n",
        "X, y = pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target)\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create a ColumnTransformer with OneHotEncoder\n",
        "preprocessor = make_column_transformer(\n",
        "    (OneHotEncoder(handle_unknown='ignore'), categorical_features),  # Apply One-Hot Encoding to categorical features\n",
        "    remainder='passthrough'  # Keep other features as they are\n",
        ")\n",
        "\n",
        "# Create an ensemble of Decision Trees using Bagging\n",
        "n_estimators = 25  # Number of trees in the ensemble\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # Use 'estimator' instead of 'base_estimator'\n",
        "    n_estimators=n_estimators,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Create a pipeline that first transforms the data and then fits the model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('bagging', bagging_clf)\n",
        "])\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Fit the model using the pipeline\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = pipeline.predict(X_val)\n",
        "y_proba = pipeline.predict_proba(X_val)  # Get probabilities for each class\n",
        "\n",
        "# Calculate uncertainty as 1 - max probability (higher uncertainty means lower confidence)\n",
        "uncertainty = 1 - np.max(y_proba, axis=1)\n",
        "\n",
        "# Create a DataFrame to hold predictions and uncertainty\n",
        "results_df = pd.DataFrame({\n",
        "    'Predicted': label_encoder.inverse_transform(y_pred),  # Convert back to original labels\n",
        "    'Uncertainty': uncertainty\n",
        "})\n",
        "\n",
        "# Identify top and bottom 10% based on uncertainty\n",
        "top_10_percent_threshold = results_df['Uncertainty'].quantile(0.9)\n",
        "bottom_10_percent_threshold = results_df['Uncertainty'].quantile(0.1)\n",
        "\n",
        "top_10_percent = results_df[results_df['Uncertainty'] >= top_10_percent_threshold]\n",
        "bottom_10_percent = results_df[results_df['Uncertainty'] <= bottom_10_percent_threshold]\n",
        "\n",
        "# Output results\n",
        "print(\"Top 10% Uncertainty Predictions:\")\n",
        "print(top_10_percent)\n",
        "\n",
        "print(\"\\nBottom 10% Uncertainty Predictions:\")\n",
        "print(bottom_10_percent)\n"
      ],
      "metadata": {
        "id": "J1gnl1Lym5mS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5d79dd7-cc75-4163-8fd5-257862eb8146"
      },
      "id": "J1gnl1Lym5mS",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10% Uncertainty Predictions:\n",
            "         Predicted  Uncertainty\n",
            "0      b'neptune.'          0.0\n",
            "1       b'normal.'          0.0\n",
            "2        b'smurf.'          0.0\n",
            "3       b'normal.'          0.0\n",
            "4       b'normal.'          0.0\n",
            "...            ...          ...\n",
            "98800   b'normal.'          0.0\n",
            "98801    b'smurf.'          0.0\n",
            "98802    b'smurf.'          0.0\n",
            "98803   b'normal.'          0.0\n",
            "98804  b'neptune.'          0.0\n",
            "\n",
            "[98805 rows x 2 columns]\n",
            "\n",
            "Bottom 10% Uncertainty Predictions:\n",
            "         Predicted  Uncertainty\n",
            "0      b'neptune.'          0.0\n",
            "1       b'normal.'          0.0\n",
            "2        b'smurf.'          0.0\n",
            "3       b'normal.'          0.0\n",
            "4       b'normal.'          0.0\n",
            "...            ...          ...\n",
            "98800   b'normal.'          0.0\n",
            "98801    b'smurf.'          0.0\n",
            "98802    b'smurf.'          0.0\n",
            "98803   b'normal.'          0.0\n",
            "98804  b'neptune.'          0.0\n",
            "\n",
            "[98673 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "400fc0b0",
      "metadata": {
        "id": "400fc0b0"
      },
      "source": [
        "# Exercise 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_kddcup99\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "# Load the KDDcup99 dataset\n",
        "data = fetch_kddcup99()\n",
        "X, y = pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target)\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create a ColumnTransformer with OneHotEncoder\n",
        "preprocessor = make_column_transformer(\n",
        "    (OneHotEncoder(handle_unknown='ignore'), categorical_features),  # Apply One-Hot Encoding to categorical features\n",
        "    remainder='passthrough'  # Keep other features as they are\n",
        ")\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Feature Selection Method 1: Using Random Forest for feature importance\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "X_train_transformed = preprocessor.fit_transform(X_train)\n",
        "\n",
        "# Fit the Random Forest model to get feature importances\n",
        "rf.fit(X_train_transformed, y_train)\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Get indices of the 10 most important features\n",
        "indices = np.argsort(importances)[-10:]\n",
        "\n",
        "# Get the names of the selected features\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "top_rf_features = feature_names[indices]\n",
        "\n",
        "# Display the 10 most important features from Random Forest\n",
        "print(\"Top 10 Features from Random Forest:\")\n",
        "print(top_rf_features)\n",
        "\n",
        "# Feature Selection Method 2: Using SelectKBest with chi-squared test\n",
        "# Note: chi-squared test requires non-negative features; we need to ensure the transformed data meets this condition.\n",
        "X_train_kbest = X_train_transformed\n",
        "\n",
        "# Apply SelectKBest\n",
        "k_best_selector = SelectKBest(score_func=chi2, k=10)\n",
        "k_best_selector.fit(X_train_kbest, y_train)\n",
        "\n",
        "# Get the mask of selected features\n",
        "k_best_mask = k_best_selector.get_support()\n",
        "top_kbest_features = feature_names[k_best_mask]\n",
        "\n",
        "# Display the 10 most important features from SelectKBest\n",
        "print(\"Top 10 Features from SelectKBest (chi-squared):\")\n",
        "print(top_kbest_features)\n",
        "\n",
        "# Combine selected features from both methods\n",
        "selected_features = np.unique(np.concatenate((top_rf_features, top_kbest_features)))\n",
        "\n",
        "# Create new training and validation sets with only selected features\n",
        "X_train_selected = X_train_transformed[:, np.isin(feature_names, selected_features)]\n",
        "X_val_selected = preprocessor.transform(X_val)[:, np.isin(feature_names, selected_features)]\n",
        "\n",
        "# Retrain the Decision Tree classifier with the selected features\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "decision_tree.fit(X_train_selected, y_train)\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "y_pred_selected = decision_tree.predict(X_val_selected)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = np.mean(y_pred_selected == y_val)\n",
        "print(f\"Accuracy of Decision Tree with selected features: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "1nI4iRE0m4-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1861a5-c09d-4a5e-ef16-6dd42120010b"
      },
      "id": "1nI4iRE0m4-T",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 Features from Random Forest:\n",
            "['onehotencoder__dst_host_serror_rate_1.0'\n",
            " 'onehotencoder__same_srv_rate_1.0' 'onehotencoder__logged_in_0'\n",
            " 'onehotencoder__dst_host_same_src_port_rate_1.0'\n",
            " \"onehotencoder__service_b'private'\" 'onehotencoder__dst_bytes_0'\n",
            " 'onehotencoder__dst_host_same_src_port_rate_0.0'\n",
            " \"onehotencoder__service_b'ecr_i'\" \"onehotencoder__protocol_type_b'tcp'\"\n",
            " \"onehotencoder__protocol_type_b'icmp'\"]\n",
            "Top 10 Features from SelectKBest (chi-squared):\n",
            "['onehotencoder__src_bytes_28' 'onehotencoder__src_bytes_1480'\n",
            " 'onehotencoder__src_bytes_54540' 'onehotencoder__dst_bytes_8127'\n",
            " 'onehotencoder__dst_bytes_8314' 'onehotencoder__land_1'\n",
            " 'onehotencoder__wrong_fragment_1' 'onehotencoder__wrong_fragment_3'\n",
            " 'onehotencoder__hot_2' 'onehotencoder__num_compromised_1']\n",
            "Accuracy of Decision Tree with selected features: 0.9934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21da71c",
      "metadata": {
        "id": "f21da71c"
      },
      "source": [
        "# Exercise 4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_kddcup99\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the KDDcup99 dataset\n",
        "data = fetch_kddcup99()\n",
        "X, y = pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target)\n",
        "\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Identify categorical features\n",
        "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),  # One-hot encode categorical features\n",
        "        ('num', StandardScaler(), X.select_dtypes(include=['float64', 'int64']).columns.tolist())  # Scale numerical features\n",
        "    ],\n",
        "    remainder='passthrough'  # Keep other features as they are\n",
        ")\n",
        "\n",
        "# Preprocess the data (fit and transform)\n",
        "X_transformed = preprocessor.fit_transform(X)\n",
        "\n",
        "# Convert to dense array\n",
        "X_dense = X_transformed.toarray() if hasattr(X_transformed, 'toarray') else X_transformed\n",
        "\n",
        "# Define clustering algorithms\n",
        "kmeans = KMeans(n_clusters=10, random_state=42)\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=10)\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
        "\n",
        "# Fit and predict clusters\n",
        "clusters_kmeans = kmeans.fit_predict(X_dense)\n",
        "clusters_agg = agg_clustering.fit_predict(X_dense)\n",
        "clusters_dbscan = dbscan.fit_predict(X_dense)\n",
        "\n",
        "# Evaluate clustering performance using silhouette score\n",
        "silhouette_kmeans = silhouette_score(X_dense, clusters_kmeans)\n",
        "silhouette_agg = silhouette_score(X_dense, clusters_agg)\n",
        "# Note: DBSCAN may produce noise (-1), so we will only compute the silhouette score for valid clusters\n",
        "if len(set(clusters_dbscan)) > 1:  # Check if there are multiple clusters\n",
        "    silhouette_dbscan = silhouette_score(X_dense[clusters_dbscan != -1], clusters_dbscan[clusters_dbscan != -1])\n",
        "else:\n",
        "    silhouette_dbscan = -1  # If there's only one cluster or noise\n",
        "\n",
        "# Output the silhouette scores\n",
        "print(f\"Silhouette Score for K-Means: {silhouette_kmeans:.4f}\")\n",
        "print(f\"Silhouette Score for Agglomerative Clustering: {silhouette_agg:.4f}\")\n",
        "print(f\"Silhouette Score for DBSCAN: {silhouette_dbscan:.4f}\")\n",
        "\n",
        "# Analyze cluster distribution for K-Means\n",
        "kmeans_df = pd.DataFrame({'Cluster': clusters_kmeans, 'True Label': y_encoded})\n",
        "kmeans_distribution = kmeans_df.groupby(['Cluster', 'True Label']).size().unstack(fill_value=0)\n",
        "\n",
        "# Visualize cluster distribution for K-Means\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(kmeans_distribution, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title(\"K-Means Cluster Distribution\")\n",
        "plt.ylabel(\"Cluster\")\n",
        "plt.xlabel(\"True Label\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YQape4EMm4dA"
      },
      "id": "YQape4EMm4dA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "a1f8a97d",
      "metadata": {
        "id": "a1f8a97d"
      },
      "source": [
        "# Exercise 5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Retrieve top and bottom 10% instances based on uncertainty (from Exercise 2)\n",
        "top_10_percent_indices = np.argsort(uncertainties)[-int(0.1 * num_samples):]  # Most uncertain\n",
        "bottom_10_percent_indices = np.argsort(uncertainties)[:int(0.1 * num_samples)]  # Least uncertain\n",
        "\n",
        "# Extract the corresponding data\n",
        "X_top_10 = X_test.iloc[top_10_percent_indices]\n",
        "X_bottom_10 = X_test.iloc[bottom_10_percent_indices]\n",
        "\n",
        "# Step 2: Cluster the Top 10% instances\n",
        "clustering_algorithms = {\n",
        "    \"KMeans\": KMeans(n_clusters=3, random_state=42),\n",
        "    \"Agglomerative Clustering\": AgglomerativeClustering(n_clusters=3),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.5, min_samples=5)\n",
        "}\n",
        "\n",
        "# Analyze clusters for Top 10% uncertain data\n",
        "top_clusters_comparison = {}\n",
        "for name, model in clustering_algorithms.items():\n",
        "    model.fit(X_top_10)\n",
        "    if name == \"DBSCAN\":\n",
        "        labels_top = model.labels_\n",
        "    else:\n",
        "        labels_top = model.predict(X_top_10)\n",
        "\n",
        "    # Store results\n",
        "    top_clusters_comparison[name] = labels_top\n",
        "\n",
        "# Analyze clusters for Bottom 10% uncertain data\n",
        "bottom_clusters_comparison = {}\n",
        "for name, model in clustering_algorithms.items():\n",
        "    model.fit(X_bottom_10)\n",
        "    if name == \"DBSCAN\":\n",
        "        labels_bottom = model.labels_\n",
        "    else:\n",
        "        labels_bottom = model.predict(X_bottom_10)\n",
        "\n",
        "    # Store results\n",
        "    bottom_clusters_comparison[name] = labels_bottom\n",
        "\n",
        "# Step 3: Examine characteristics of the clusters formed\n",
        "def analyze_clusters(X_data, labels, title):\n",
        "    cluster_df = pd.DataFrame({'Cluster': labels})\n",
        "    cluster_counts = cluster_df['Cluster'].value_counts().sort_index()\n",
        "    print(f\"\\n{title} Cluster Sizes:\")\n",
        "    print(cluster_counts)\n",
        "\n",
        "    # Optionally, display a sample of instances from each cluster\n",
        "    for cluster in cluster_counts.index:\n",
        "        print(f\"\\nSample Instances from Cluster {cluster}:\")\n",
        "        print(X_data[cluster_df['Cluster'] == cluster].head())\n",
        "\n",
        "# Analyze clusters for the Top 10% uncertain data\n",
        "print(\"\\nAnalyzing clusters for Top 10% uncertain data:\")\n",
        "for name, labels in top_clusters_comparison.items():\n",
        "    analyze_clusters(X_top_10, labels, f\"Top 10% - {name}\")\n",
        "\n",
        "# Analyze clusters for the Bottom 10% uncertain data\n",
        "print(\"\\nAnalyzing clusters for Bottom 10% uncertain data:\")\n",
        "for name, labels in bottom_clusters_comparison.items():\n",
        "    analyze_clusters(X_bottom_10, labels, f\"Bottom 10% - {name}\")"
      ],
      "metadata": {
        "id": "DaGFitTZm7N4"
      },
      "id": "DaGFitTZm7N4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1337979f",
      "metadata": {
        "id": "1337979f"
      },
      "source": [
        "# Exercise 6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_kddcup99\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Step 1: Load the KDDcup99 dataset\n",
        "data = fetch_kddcup99()\n",
        "X, y = pd.DataFrame(data.data, columns=data.feature_names), pd.Series(data.target)\n",
        "\n",
        "# Step 2: Select Features and Check Dimensions\n",
        "num_features = X.shape[1]  # Number of features in KDDcup99\n",
        "print(f\"Original number of features: {num_features}\")\n",
        "\n",
        "# Step 3: Ensure Features are Discrete or Continuous\n",
        "# Convert categorical features to numerical values using Label Encoding or One-Hot Encoding\n",
        "for col in X.select_dtypes(include=['object']).columns:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# Convert all feature types to integers or floats\n",
        "X = X.astype(float)  # Convert to float for consistency\n",
        "\n",
        "# Check the new dimensionality\n",
        "new_num_features = X.shape[1]  # Number of features after transformation\n",
        "print(f\"New number of features: {new_num_features}\")\n",
        "\n",
        "# Step 4: Create Target Variable\n",
        "# Target should be 'normal.' or the name of the anomaly type\n",
        "# Map the original target to 'normal.' and anomaly types\n",
        "y = y.apply(lambda x: 'normal.' if x == 'normal.' else x)  # Retain anomaly names\n",
        "\n",
        "# Combine Features and Target\n",
        "final_dataset = pd.concat([X, y.rename('target')], axis=1)\n",
        "\n",
        "# Step 5: Output the New Structure\n",
        "samples_total = final_dataset.shape[0]\n",
        "dimensionality = final_dataset.shape[1] - 1  # Exclude target column\n",
        "\n",
        "print(f\"Samples total: {samples_total}\")\n",
        "print(f\"Dimensionality: {dimensionality}\")\n",
        "print(f\"Features: {X.dtypes}\")\n",
        "\n",
        "# Export the transformed dataset to CSV\n",
        "final_dataset.to_csv('SA.csv', index=False)"
      ],
      "metadata": {
        "id": "uPmLrIsF7Phz"
      },
      "id": "uPmLrIsF7Phz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, LeaveOneOut\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the \"SA\" dataset (Assuming it's in CSV format)\n",
        "# Replace 'path_to_sa_dataset.csv' with the actual path to the dataset\n",
        "sa_data = pd.read_csv('SA.csv')\n",
        "\n",
        "# Step 2: Preprocess the dataset\n",
        "# Here, you may need to define your feature columns and label (if applicable)\n",
        "# For example, if your dataset has a label column named 'label':\n",
        "features = sa_data.drop(columns=['label'], errors='ignore')  # Assuming 'label' indicates normal/anomaly\n",
        "labels = sa_data['label'] if 'label' in sa_data.columns else None\n",
        "\n",
        "# Step 3: Create a subsample of 250 datapoints\n",
        "sa_sample = features.sample(n=250, random_state=42)\n",
        "\n",
        "# Step 4: Perform Leave-One-Out Cross-Validation for each algorithm\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Define anomaly detection algorithms\n",
        "anomaly_detectors = {\n",
        "    \"Isolation Forest\": IsolationForest(contamination=0.1, random_state=42),\n",
        "    \"One-Class SVM\": OneClassSVM(gamma='auto', nu=0.1),  # Adjust 'nu' as needed\n",
        "    \"Local Outlier Factor\": LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n",
        "}\n",
        "\n",
        "# Initialize performance report\n",
        "performance_report = {}\n",
        "\n",
        "# Evaluate each algorithm using LOOCV\n",
        "for name, detector in anomaly_detectors.items():\n",
        "    y_true = []  # True labels (for comparison, if available)\n",
        "    y_pred = []  # Predicted labels\n",
        "\n",
        "    for train_index, test_index in loo.split(sa_sample):\n",
        "        X_train, X_test = sa_sample.iloc[train_index], sa_sample.iloc[test_index]\n",
        "\n",
        "        if name == \"Local Outlier Factor\":\n",
        "            # LOF does not need to fit on the test point\n",
        "            detector.fit(X_train)\n",
        "            y_pred.append(detector.fit_predict(X_test)[0])  # LOF returns -1 for outliers and 1 for inliers\n",
        "        else:\n",
        "            detector.fit(X_train)\n",
        "            y_pred.append(detector.predict(X_test)[0])  # Predict for the test point\n",
        "\n",
        "        # Record the true label if available\n",
        "        if labels is not None:\n",
        "            y_true.append(labels.iloc[test_index[0]])\n",
        "\n",
        "    # Convert predictions to binary (1: inliers, -1: outliers)\n",
        "    y_pred_binary = [1 if p == -1 else 0 for p in y_pred]  # Convert outlier predictions to binary\n",
        "\n",
        "    # If true labels are available, generate classification report\n",
        "    if labels is not None:\n",
        "        print(f\"\\nClassification Report for {name}:\")\n",
        "        print(classification_report(y_true, y_pred_binary))\n",
        "    performance_report[name] = y_pred_binary  # Store predictions for later use\n",
        "\n",
        "# Optional: Analyze and visualize the predictions for each detector\n",
        "# This part can be customized based on the specific needs of the analysis\n"
      ],
      "metadata": {
        "id": "AJ9VeNn_m75X"
      },
      "id": "AJ9VeNn_m75X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ae66db91",
      "metadata": {
        "id": "ae66db91"
      },
      "source": [
        "# Exercise 7 (Redo Exercise 6 with Leave One Out)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X2siuH8rm8V1"
      },
      "id": "X2siuH8rm8V1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ac2a9b0a",
      "metadata": {
        "id": "ac2a9b0a"
      },
      "source": [
        "# Exercise 8"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1GY7CXDWm843"
      },
      "id": "1GY7CXDWm843",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Results\n"
      ],
      "metadata": {
        "id": "TEoXhHmRmv-5"
      },
      "id": "TEoXhHmRmv-5"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "laqScUg1m-M5"
      },
      "id": "laqScUg1m-M5"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}